---
layout: single 
title: "Randomness" 
category: story
permalink: /weblog/topics/information/theory/randomness_morowitz_citation_2007.html
tags: [information, information theory] 
comments: false 
author: John Hawks 
---


<p>
From a passage on the statistical behavior of aggregates and probability theory, p. 64-65 in <i>Entropy for Biologists</i> by Harold J. Morowitz, Academic Press, New York, 1970:
</p>

<blockquote>The notion of randomness is a very important one in physics, yet difficult to describe. (Randomness has become so significant that one of the outstanding scientific publications of recent years was a book of one million random digits.) Often a process is so complicated or we are so ignorant of the boundary conditions, or of the laws governing the process, that we are unable to predict the result of the process in any but a statistical fashion. For instance, suppose we have a collection of radioactive phosphorus atoms, P<sup>32</sup>, and take an individual atom and question how long it will take to emit an electron. Here we do not know the boundary conditions, i.e., the detailed state of the nucleus, nor do we know the exact laws coverning radioactive decay. The time can take on any value. We may obtain an aggregate of such values as is done in experiments on radioactive half-lives and deduce certain features of the collection, but we may only make probability statements about the individual atom. Randomness is in a certain sense a consequence of the ignorance of the observer, yet randomness itself displays certain properties which have been turned into powerful tools in the study of the behavior of systems of atoms. </blockquote>

