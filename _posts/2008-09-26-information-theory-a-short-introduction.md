---
layout: single 
title: "Information theory: a short introduction" 
category: story
permalink: /weblog/topics/information/theory/information-theory-intro-2008.html
tags: [information, HapMap, snps, information theory] 
comments: false 
author: John Hawks 
---

<p class="noindent" >I lectured this week in my <a  href="http://biologyofmind.johnhawks.net/" >Biology of Mind course</a> about information theory, and in particular the concept of Shannon entropy. I&#8217;ve typed up a few notes for my students, and I&#8217;m cross-posting them on my own blog because they are relevant to another topic I&#8217;ll be writing about: discovery and testing of natural selection in the human genome. You see, the kind of data that are presently being collected as part of the International HapMap , single nucleotide polymorphisms (SNPs), are naturally treated by information theoretic measures. So first, it may help to define the essential concepts of information theory. </p>
<!--break-->
<p class="indent" >   Many readers will have heard of the concept of <span  class="cmbx-10">entropy </span>in connection to the thermodynamics of physical systems. Indeed, one common statement of the <a  href="http://en.wikipedia.org/wiki/Second_law_of_thermodynamics" >Second Law of Thermodynamics</a> is that a closed system must increase in entropy over time. Entropy is a statistical characteristic of a system, related to the probabilities that the particles of a system will be found in given states at any given time. In thermodynamic terms, a system&#8217;s entropy is related to our ability to extract work from the system. The Second Law implies that work cannot endlessly be extracted from a closed system without the addition of energy from outside. By 1927, <a  href="http://en.wikipedia.org/wiki/Leo_Szilard" >Le&oacute; Szil&aacute;rd</a> (probably my favorite physicist) had shown that the entropy of a physical system can be naturally defined in terms of <span  class="cmbx-10">information</span>. In other words, one way of looking at entropy is in terms of uncertainty about the state of any given particle in a system, and one might apply energy to a system in order to reduce this uncertainty (for example, by concentrating particles in one part of the system. </p>

<p class="indent" >   <a  href="http://en.wikipedia.org/wiki/Claude_Shannon" >Claude Shannon</a> developed the concept of entropy as applied to communication systems. By doing so, he established the field of information theory. Shannon developed several fundamental theorems, including a derivation of the relation between channel capacity (bandwidth) and noise, studies of optimal encoding strategies, and a means of treating continuous as well as finite communications. His most basic definition is that of information entropy, which has also come to be called the Shannon entropy. This definition places entropy as a measure of our <span  class="cmbx-10">uncertainty </span>about the state of a system&#8212;in particular, as applied to information, a system of signs. </p>

<p class="indent" >   Shannon published &#8220;A mathematical theory of communication&#8221; in 1947, describing his theoretical work. Along with this article, the (UW-Madison) mathematician Warren Weaver wrote a popular treatment of Shannon&#8217;s work, titled &#8220;Recent contributions to the mathematical theory of communication.&#8221; I mention these articles because it is very hard to improve upon them; they are clear in their exposition and notation. The two can be found together in the 1948 book, <a  href="http://www.amazon.com/gp/product/0252725484?ie=UTF8&tag=johnhawksanth-20&linkCode=as2&camp=1789&creative=9325&creativeASIN=0252725484" >The Mathematical Theory of Communication</a>, which has been reprinted several times. I                                                                                                                                      can&#8217;t recommend this book highly enough. </p>

<p class="indent" >   Keeping that in mind, I won&#8217;t be reiterating the essentials of information theory here; I just want to give a basic understanding that can be applied to other problems&#8212;particularly with respect to SNP datasets. <a   id="section*.2"></a></p>

    <h3 class="sectionHead"><a   id="x1-2000"></a>Entropy and outcomes</h3> <p class="noindent" >Suppose we have an electron in a box. Its <span  class="cmbx-10">spin </span>may be &#8220;left&#8221; or &#8220;right&#8221;, with equal probability. What is our <span  class="cmbx-10">uncertainty </span>about the electron&#8217;s spin? One way of looking at the question: We are just as uncertain about the electron as we would be about the flip of a fair coin. The uncertainty in both cases has the same <span  class="cmbx-10">quantity</span>, even though the systems are in other respects totally different from each other. Hence, it is desirable that our definition of uncertainty not depend on the actual physical characteristics of a system, but only upon the <span  class="cmbx-10">probabilities </span>of signs within the system. </p>

<p class="indent" >   If we were not uncertain at all, the probability of one outcome would be 1 and the other would be zero. For instance, if we had a two-headed coin, we would be absolutely certain of flipping heads. Naturally, a definition of uncertainty should assign zero to the case in which we already know the outcome. But for a fair coin, we have a probability of 0.5 for one outcome, and a probability of 0.5 for the other. We are uncertain, and our measure of uncertainty should have a positive value in this case, whatever unit we may choose. </p>

<p class="indent" >   Now suppose we have a nucleotide of DNA. It may be adenine, guanine, cytosine or thymine, each with probability 0.25 (1/4). How many coin flips would give us the same amount of uncertainty? The answer is two: Two flips have four possible outcomes (0,0; 0,1; 1,0; 1,1) with equal probability (0.25) for each. Again we have an equivalence between two systems in the amount of uncertainty about the outcome. However, in this case we can see that it takes two trials of one system to attain the same uncertainty as one trial of the other system. It would seem that we should be <span  class="cmti-10">twice </span>as uncertain about the nucleotide as we are about a single coin flip. Indeed, three nucleotides of DNA (a codon) will give us 64 possible outcomes&#8212;the same as six coin flips. </p>

<p class="indent" >   The number of possible outcomes of a set of trials grows as the <span  class="cmti-10">exponent </span>of the number of trials. So for example, 5 coin flips yield 2<sup><span  class="cmr-7">5</span></sup> = 32 possible outcomes; 10 coin flips yield 2<sup><span  class="cmr-7">10</span></sup> = 1024 possible outcomes. If the coin is fair, then every outcome is equally probable; meaning that the probability of any sequence of 10 coin results might be observed with probability 1/1024. A consideration of this system over a slightly larger scale will give some idea of the power of encoding. With 10 coin flip results, we might choose a room in a 1024-bed hospital at random. With 100 coin flips, we may describe a system of 1<span  class="cmmi-10">.</span>2 <span  class="cmsy-10">&#x00D7; </span>10<sup><span  class="cmr-7">30</span></sup> elements&#8212;enough to randomly choose a point on the Earth&#8217;s surface to within a millionth of an inch.                                                                                                                                      </p>

<p class="indent" >   If we are uncertain where we have hidden our microdot, a hyper-GPS could communicate its location anywhere in the world to us with a string of 100 heads or tails. The <span  class="cmti-10">information </span>that will remove our uncertainty is related to the <span  class="cmti-10">logarithm </span>of the number of outcomes. This leads to a mathematical definition of uncertainty in terms of logarithms. In particular, for a system <span  class="cmmi-10">X </span>with possible outcomes <span  class="cmmi-10">x</span><sub><span  class="cmr-7">1</span></sub><span  class="cmmi-10">,x</span><sub><span  class="cmr-7">2</span></sub><span  class="cmmi-10">,</span><span  class="cmmi-10">&#x2026;</span><span  class="cmmi-10">,x</span><sub><span  class="cmmi-7">n</span></sub>, the information entropy (<span  class="cmmi-10">H</span>(<span  class="cmmi-10">X</span>)) is: </p>

    <table  class="equation"><tr><td>    <center class="math-display" > <img  src="/graphics/information-theory-intro-20080x.png" alt="          &#x2211;n H (X ) = -   p(xi)logp(xi)           i=1 " class="math-display"  /><a   id="x1-2001r1"></a></center></td><td class="equation-label">(1)</td></tr></table> <p class="nopar" > </p>

<p class="noindent" >The logarithm is conventionally taken as a base-2 logarithm, so that the measure of entropy is the <span  class="cmti-10">binary digit</span>, or <span  class="cmbx-10">bit</span>. A single coin flip has two possible outcomes each with probability 0.5. The equation gives us: </p>

    <table  class="equation"><tr><td>    <center class="math-display" > <img  src="/graphics/information-theory-intro-20081x.png" alt="H (coin flip) = - [0.5 log0.5+ 0.5log 0.5] = 1 bit " class="math-display"  /><a   id="x1-2002r2"></a></center></td><td class="equation-label">(2)</td></tr></table> <p class="nopar" > </p>

<p class="noindent" >This equation allows us also to handle systems in which the probabilities of different outcomes are not all equal. For example, suppose there are two outcomes, with probability 0.9 and 0.1. What is the uncertainty?                                                                                                                                      </p>

    <table  class="equation"><tr><td>    <center class="math-display" > <img  src="/graphics/information-theory-intro-20082x.png" alt="H (X ) = - [0.9 log0.9+ 0.1log 0.1] = 0.47 bits " class="math-display"  /><a   id="x1-2003r3"></a></center></td><td class="equation-label">(3)</td></tr></table> <p class="nopar" > </p>

<p class="indent" >   Here we are less than half as uncertain as in the case of a fair coin&#8212;and indeed, that is the point. If we had a coin that consistently gave only 10% tails, we would on average be considerably more certain about the outcome. On average, we can communicate two flips of our unfair coin with only one bit. Exactly <span  class="cmti-10">how </span>this can be done is by using the right kind of <span  class="cmti-10">encoding</span>. The point for us is that a system with much less uncertainty than a coin flip can be specified with <span  class="cmti-10">less information </span>than a coin flip. <a   id="section*.3"></a></p>

    <h3 class="sectionHead"><a   id="x1-3000"></a>Mutual information</h3> <p class="noindent" >Now, suppose we have two distinct events. If these events are independent, then the <span  class="cmbx-10">joint entropy </span>represented by both is simply the sum of their individual entropies: </p>

    <table  class="equation"><tr><td>    <center class="math-display" > <img  src="/graphics/information-theory-intro-20083x.png" alt="H (X,Y ) = H (X)+ H (Y) " class="math-display"  /><a   id="x1-3001r4"></a></center></td><td class="equation-label">(4)</td></tr></table> <p class="nopar" >                                                                                                                                      </p>

<p class="indent" >   Why is this? Consider two coin flips. In the <span  class="cmti-10">combined </span>system including both flips, we have four possible outcomes (0,0; 0,1; 1,0; 1,1). If our two flips are independent, then the probability of each <span  class="cmti-10">combined </span>outcome is the product of the probabilities of the <span  class="cmti-10">individual </span>outcomes. That is, <span  class="cmmi-10">p</span>(0<span  class="cmmi-10">,</span>1) = <span  class="cmmi-10">p</span>(0)<span  class="cmmi-10">p</span>(1), and log <span  class="cmmi-10">p</span>(0)<span  class="cmmi-10">p</span>(1) = log <span  class="cmmi-10">p</span>(0) + log <span  class="cmmi-10">p</span>(1). Then, equation <a  href="#x1-3001r4">4</a> can be derived from <a  href="#x1-2001r1">1</a> by algebraic manipulation. </p>

<p class="indent" >   But, if the two events are <span  class="cmti-10">not </span>independent&#8212;that is, if the outcome of one <span  class="cmti-10">depends on </span>the outcome of the other, then their joint entropy must be <span  class="cmti-10">less than </span>the sum of their individual entropies. </p>

    <table  class="equation"><tr><td>    <center class="math-display" > <img  src="/graphics/information-theory-intro-20084x.png" alt="H (X,Y ) &#x2264; H (X)+ H (Y) " class="math-display"  /><a   id="x1-3002r5"></a></center></td><td class="equation-label">(5)</td></tr></table> <p class="nopar" > </p>

<p class="noindent" >And the <span  class="cmti-10">difference </span>between the joint entropy and the sum of the individual entropies is a measure of the <span  class="cmti-10">correlation </span>between the two events. We call this difference the <span  class="cmbx-10">mutual information </span>of the two events, and we define it mathematically: </p>

    <table  class="equation"><tr><td>    <center class="math-display" > <img  src="/graphics/information-theory-intro-20085x.png" alt="I(X;Y ) = H (X)+ H (Y)- H (X,Y ) " class="math-display"  /><a   id="x1-3003r6"></a></center></td><td class="equation-label">(6)</td></tr></table> <p class="nopar" >                                                                                                                                      </p>

<p class="indent" >   Consider a game of Blackjack at a casino. Most people play probabilities as if every card were equally likely to be dealt in every hand. Under this scenario, the house has a consistent edge&#8212;this is, after all, how casinos make money. But in fact every card is <span  class="cmti-10">not </span>equally likely to be dealt. In particular, there is a <span  class="cmti-10">serial correlation</span> among the cards dealt in a Blackjack game. If the King of Hearts is dealt, it is rather less likely to occur again very quickly. In other words, there is <span  class="cmbx-10">mutual</span> <span  class="cmbx-10">information </span>between the dealing of a card and the outcomes of later hands: Once the King of Hearts is observed, its probability of being dealt in later hands declines. A clever player with a good memory may make use of this mutual information to guide his bets: putting down more money when the house is less likely to draw face cards, for instance. Players who can &#8220;count cards&#8221; in this way would be the doom of the casinos if they allowed it to continue. To prevent it, they reduce the extent of serial correlations by dealing cards from boots of four or more decks, and the casinos eject players suspected of counting. <a   id="section*.4"></a></p>

    <h3 class="sectionHead"><a   id="x1-4000"></a>Redundancy</h3> <p class="noindent" >Moreover, a system comprised of two distinct events may include <span  class="cmbx-10">redundancy</span>. We may consider a very prominent system in which two independent events, each with two outcomes, give rise to a combined system with only <span  class="cmti-10">three</span>, not four outcomes. Consider a Mendelian gene <span  class="cmmi-10">A </span>with two alleles <span  class="cmmi-10">a</span><sub><span  class="cmr-7">1</span></sub> and <span  class="cmmi-10">a</span><sub><span  class="cmr-7">2</span></sub>. When two heterozygotes (<span  class="cmmi-10">a</span><sub><span  class="cmr-7">1</span></sub><span  class="cmmi-10">a</span><sub><span  class="cmr-7">2</span></sub>) mate, their offspring will have one of <span  class="cmti-10">three </span>genotypes: <span  class="cmmi-10">a</span><sub><span  class="cmr-7">1</span></sub><span  class="cmmi-10">a</span><sub><span  class="cmr-7">1</span></sub>, <span  class="cmmi-10">a</span><sub><span  class="cmr-7">1</span></sub><span  class="cmmi-10">a</span><sub><span  class="cmr-7">2</span></sub>, or <span  class="cmmi-10">a</span><sub><span  class="cmr-7">2</span></sub><span  class="cmmi-10">a</span><sub><span  class="cmr-7">2</span></sub>. If we want to communicate the genotypes of their children, we will need an average of 1.5 bits for each. </p>

<p class="indent" >   This seems counter-intuitive, considering that each <span  class="cmti-10">gamete </span>from the parents is <span  class="cmmi-10">a</span><sub><span  class="cmr-7">1</span></sub> or <span  class="cmmi-10">a</span><sub><span  class="cmr-7">2</span></sub> with equal probability. That is, <span  class="cmmi-10">p</span>(<span  class="cmmi-10">a</span><sub><span  class="cmr-7">1</span></sub>) = <span  class="cmmi-10">p</span>(<span  class="cmmi-10">a</span><sub><span  class="cmr-7">2</span></sub>) = 0<span  class="cmmi-10">.</span>5. The entropy represented by each gamete is a coin flip&#8217;s worth&#8212;one bit. It might seem that combining two gametes, each derived independently from a different parent, we should have 2 bits of entropy, not only 1.5. </p>

<p class="indent" >   Yet it is a simple matter to show that we can <span  class="cmti-10">transmit </span>information about the children&#8217;s genotypes using only 1.5 bits. Let&#8217;s encode a heterozygote as a &#8220;0&#8221;, an <span  class="cmmi-10">a</span><sub><span  class="cmr-7">1</span></sub> homozygote as &#8220;10&#8221; and an <span  class="cmmi-10">a</span><sub><span  class="cmr-7">2</span></sub> homozygote as &#8220;11&#8221;. In this encoding, a single bit communicates whether the child is a homozygote or heterozygote, and <span  class="cmti-10">if homozygote</span> is followed by a second bit communicating which of the two alleles. Now, if our couple of heterozygotes have eight children, four of whom are heterozygotes and two of each homozygote, we can transmit their family&#8217;s genotypes as &#8220;000010101111&#8221;. Eight children. Twelve bits. That&#8217;s 1.5 bits per genotype. In some unlikely cases (all homozygotes) we will use more bits, in others (all heterozygotes) we will use less. </p>

<p class="indent" >   In this case, two different outcomes from the point of view of inheritance are in fact not distinguished in the genotype of each child. &#8220;Heterozygotes&#8221; include two distinct classes: those who inherit <span  class="cmmi-10">a</span><sub><span  class="cmr-7">1</span></sub> from the mother (and <span  class="cmmi-10">a</span><sub><span  class="cmr-7">2</span></sub> from the father) and those who inherit <span  class="cmmi-10">a</span><sub><span  class="cmr-7">2</span></sub> from the mother (and <span  class="cmmi-10">a</span><sub><span  class="cmr-7">1</span></sub> from the father). The system that                                                                                                                                      gives rise to the genotypes is relevant to the probability that each genotype will occur (as Mendel discovered), but it is <span  class="cmti-10">not </span>very relevant to the way we <span  class="cmti-10">describe</span> those genotypes. All we know about heterozygotes is that they have two different alleles. When we want to predict phenotypes, we may not care which allele came from which parent. When we collect genotypes (on an Affy or Illumina chip, for example), we are in a poor position to <span  class="cmti-10">determine </span>which allele came from which parent. Thus, even though two bits of gametes went into the physical system creating the genotypes, only 1.5 bits is sufficient to communicate them. </p>

<p class="indent" >   What is the import of this redundancy? Clearly, if we are <span  class="cmti-10">interested </span>in children&#8217;s genotypes, then a system that tracks their parents&#8217; gametic contributions is a poor way of encoding the information. That system includes redundant information, with respect to genotypes. But to put the problem another way, knowing the children&#8217;s genotypes leaves us with <span  class="cmti-10">uncertainty </span>about their parents&#8217; gametic contributions. If we want to devise an accurate paternity test, we will <span  class="cmti-10">sometimes </span>need to know more than the child&#8217;s genotype. </p>

<p class="indent" >   <span  class="cmbx-10"><a href="http://johnhawks.net/weblog/topics/information/theory/chi-square-mutual-information-2008.html">Next: Information theory and mutual information between genetic loci</a></span> </p>

