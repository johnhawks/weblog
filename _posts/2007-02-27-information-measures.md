---
layout: single 
title: "Information measures" 
category: story
permalink: /weblog/topics/information/theory/information_measures_morowitz_citation_2007.html
tags: [information, information theory] 
comments: false 
author: John Hawks 
---


<p>
Pp. 66-67 in <i>Entropy for Biologists</i> by Harold J. Morowitz, Academic Press, New York, 1970 (emphasis added):
</p>

<blockquote>The logic of our approach may be difficult to follow since information is not a physical quantity in the sense that mass, charge, or pressure are physical quantities. Information deals with the usefulness of a set of symbols to an observer. <b>Since information does not measure anything physical, we are free to choose any information measure we please.</b> The definition is therefore at first arbitrary and the choice is based on a common sense estimate of the usefulness of a set of symbols. The original definition arixing from the needs of the communications industry was, to use P. W. Bridgman's words, "of such unblushing economic tinge." What in the end turns out to be surprising is that the definition which was introduced is found to relate to the entropy concept in interesting and very fundamental ways.</blockquote>

