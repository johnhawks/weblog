---
layout: single 
title: "For all those fake papers you&#39;ve written" 
category: story
permalink: /weblog/topics/information/fake_paper_detector_2006.html
tags: [information] 
comments: false 
author: John Hawks 
---


<p>
From <a href="http://www.newscientist.com/blog/technology/2006/04/fake-paper-detector.html">the New Scientist technology blog</a> (via <a href="http://slashdot.org/article.pl?sid=06/04/25/1912226&from=rss">Slashdot</a>):
</p>

<blockquote>You may remember the story of some cheeky MIT students who wrote a computer programme to generate scientific papers. Well, now some researchers at the Indiana University School of Informatics have come up with an Inauthentic Paper Detector to foil it.</blockquote>

<blockquote>Mehmet Dalkilic, a data mining expert explains how it works: "We believe that there are subtle, short- and long-range word or even word string repetitions that exist in human texts, but not in many classes of computer-generated texts that can be used to discriminate based on meaning."</blockquote>

<p>
What is interesting is that these "subtle long-range repetitions" are definitely part of our comprehension of a text, but we don't necessarily have the confidence to claim a text is fake if it lacks them. We have the statistical sense innately that the computer in this program is making explicit. 
</p>

<p>
It is one of the many ways that we help ourselves to make language more comprehensible -- a certain redundancy that keys the mind back to the subject at hand. A good writer uses those repeated phrases to make the text more understandable. 
</p>

<p>
And it is one of the many reasons why natural texts have a relatively low information content, at least for their length -- they consistently follow certain patterns. For our minds, that's a good thing! It lets us understand them. 
</p>

